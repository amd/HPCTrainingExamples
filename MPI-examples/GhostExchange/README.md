# MPI Examples: Ghost Exchange

Welcome to the Ghost Exchange example directory. This example is intended to be a simplified istance of what an actual scientific application code using the message passing interface (MPI) might look like. We consider a problem that is discretized on a structured Cartesian grid where the physical fields are defined on a cell-wise fashion, meaning that there is a 1-1 correspondance between the computational cells used to discretize the domain and the discrete values of the solution field: the solution is defined on the cells. MPI is used to execute the necessary computations in parallel, having multiple processes handle partitions of the initial computational domain and carried out the necessary work in parallel on their assigned partition: each process is assigned a subset of the computational domain, which means a subset of the total number of cells used to discretize the domain. As the reader can guess, the problem has to be designed and implemented in a way that such parallel computation is indeed possible and correct in terms of the final answer, which means that the solution obtained in serial needs to be the same (up to machine precision) as the one obtained in parallel using MPI. From a mathematical standpoint, the parallel computation in the case we are discussing relies on the so called <i>ghost cells</i> which are cells assigned to a given process that are instead own by a different one: this means that a process can read the information associated with the ghost cells but cannot modify it. Hence, the partitoning of the computational domain among processes is actually overlapping, due to the presence of the ghost cells. The ghost cells exist because mathematical operators in discretized form need information on the neighboring cells to compute the values of physical fields on a given cell. Ghost cells surround the cells owned by a process therefore forming a <i>halo</i> around the subdomain owned by the process. Depending on the depth of the mathematical operator, the halo might be one or several layers thick. To correctly compute the value of the physical fields on the cells owned by a given process, the information on the ghost cells needs to be used, but it also needs to be used with the correct values. These correct values need to be transfered from the process that owns the ghost cells, to the process that is only reading their information. This is done through a so called <i>halo updated</i> which is an operation done using MPI where data is exchanged between processes to make sure that data is current by the time it is used to update the values of the physical fields on the owned cells. We call this example Ghost Exchange because MPI is used to perform the exchange of information on the ghost cells through the implementation of halo updates.

In this directory, there are three version of a Ghost Exchange example:
1. `GhostExchange_ArrayAssign`: this version considers a 2D domain where parallelism is further enhanced with the use of OpenMP. OpenMP is used to perform the necessary computations in parallel. MPI is used to perform the data exchange.
2. `GhostExchange_ArrayAssign_HIP`: this version also considers a 2D domain but HIP is used in place of OpenMP to further enchance parallelism. MPI is used for data exchange.
3. `GhostExchange3D_ArrayAssign`: this version is similar to `GhostExchange_ArrayAssign` but instead considers a 3D domian.

Please explore the single directories for further details on the code and various subversions.
